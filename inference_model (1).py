# -*- coding: utf-8 -*-
"""inference-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Skshamim02/9e849d2f6863787f943c4303010c02b4/inference-model.ipynb
"""

!pip install datasets

!pip install fasttext

import json
from tqdm import tqdm
import pandas as pd
tqdm.pandas()
import numpy as np

import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding,Bidirectional,Concatenate
from keras.optimizers import *

from tensorflow.keras.utils import to_categorical

import pickle
from datasets import load_metric

from nltk.translate.bleu_score import corpus_bleu
import fasttext
import tqdm as notebook_tqdm

from google.colab import drive
drive.mount('/content/drive')

# Read the training data from JSON file
train_data = pd.read_json("/content/drive/MyDrive/internship/asm_train.json", lines=True)
test_data=pd.read_json("/content/drive/MyDrive/internship/asm_test.json", lines=True)

#converts the JSON data into a Pandas DataFrame
def correct_json_format(json_file_path):
    with open(json_file_path, 'r', encoding='utf-8') as file:
        json_data = file.read()

    # Process the JSON string to insert commas between objects
    json_data = json_data.replace('}\n{', '},{')
    json_list = json.loads(f'[{json_data}]')
    df = pd.json_normalize(json_list)

    return df

df_train = correct_json_format("/content/drive/MyDrive/internship/asm_train.json")
df_test = correct_json_format("/content/drive/MyDrive/internship/asm_test.json")

df_train

df_test

#subset train DataFrame
subset_df_train=df_train.sample(frac=0.3).reset_index(drop=True)
subset_df_train.shape

df_train=subset_df_train
df_train.shape

#start symbol "\t" at the beginning of the text and an end symbol "\n" at the end of the text(target_text)
def add_start_end(target_text):
  text = "\t" + target_text + "\n"
  return text

#the DataFrame df_train will have a new column 'target_ass' containing the original text values from the 'native word' column with a start symbol "\t" at the beginning and an end symbol "\n" at the end of each text value.
df_train['target_ass'] = df_train['native word'].progress_apply(add_start_end)
df_train.head()

# add only the end token
# add_end takes a string target_text as input and adds an end symbol "\n"
def add_end(target_text):
  text = target_text + "\n"
  return text

#adds a new column named 'decoder_target' to the DataFrame df_train. The values in this new column are derived from the values in the 'native word' column.
df_train['decoder_target'] = df_train['native word'].progress_apply(add_end)
df_train.head()

def tokenize(lang):
# Create tokenizer
    #(filters)means that all characters in the input text will be considered for tokenization,(char_lvl)each character will be treated as a separate token.
    tokenizer = Tokenizer(filters='',char_level=True)
    # Fit texts
     #creates the vocabulary based on the characters present in the corpus.
    tokenizer.fit_on_texts(lang)
    word_index = tokenizer.word_index
    return tokenizer,word_index

# Tokenize words
roman_words = df_train['english word']
ass_words = df_train['target_ass']

roman_tokenizer, roman_tokens = tokenize(roman_words)
ass_tokenizer, ass_tokens = tokenize(ass_words)


print(roman_tokens)

print("No.of unique input tokens:", len(roman_tokens))

roman_words[15]

print(ass_tokens )
print("No.of unique output tokens:", len(ass_tokens))

# vocab size (no. of unique characters for each script)

roman_vocab = len(roman_tokens)+1
ass_vocab = len(ass_tokens)+1
roman_vocab,ass_vocab

roman_words[45],ass_words[45]

# convert words to int sequence

roman_words_in_ids = roman_tokenizer.texts_to_sequences(roman_words)
ass_words_in_ids = ass_tokenizer.texts_to_sequences(ass_words)

roman_words_in_ids[45],ass_words_in_ids[45]

roman_words_in_ids[5]

roman_words[5]

ass_words_in_ids[5]

# y_train
# only with end token

label_words = df_train["decoder_target"]
print(label_words[6])
#convert the text sequences in label_words into sequences of integers
y_train = ass_tokenizer.texts_to_sequences(label_words)
y_train

y_train[9]

print(label_words[9])

# max sequence length

max_encoder_seq_length_roman = df_train['english word'].str.len().max()
max_decoder_seq_length_ass = df_train['target_ass'].str.len().max()

max_encoder_seq_length_roman, max_decoder_seq_length_ass

def post_padding(data_in_int_seq, max_seq_len):
  return pad_sequences(data_in_int_seq, max_seq_len, padding = "post")
# padding

roman_padded = post_padding(roman_words_in_ids, max_encoder_seq_length_roman)
ass_padded = post_padding(ass_words_in_ids, max_decoder_seq_length_ass)
y_train_padded = post_padding(y_train, max_decoder_seq_length_ass)

print(roman_padded[16], len(roman_padded[16]))
print(ass_padded[16], len(ass_padded[16]))
print(y_train_padded[16],len(y_train_padded[16]))

latent_dim = 64
emb_dim = 100

# Define the encoder model
encoder_inputs = Input(shape=(max_encoder_seq_length_roman,), name="encoder_input")
emb_encoder = Embedding(roman_vocab, emb_dim, mask_zero=True, name="encoder_embedding")(encoder_inputs)
#encoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=False,return_state=True, name="encoder_bilstm_1"))(emb_encoder)
encoder_outputs, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=False),name="BiLSTM",merge_mode="mul")(emb_encoder)
#encoder_lstm_final = Bidirectional(LSTM(latent_dim, return_state=True, name="encoder_bilstm_2"))(encoder_lstm)
#encoder_states = [encoder_lstm_final[0], encoder_lstm_final[1]]

# concatenate along the last dimension
encoder_h = Concatenate(axis=-1,name="concat_h")([forward_h, backward_h])
encoder_c = Concatenate(axis=-1,name="concat_c")([forward_c, backward_c])
encoder_h, encoder_c

encoder_states = [encoder_h,encoder_c]
encoder_states

# decoder with two lstm layers

decoder_inputs = keras.Input(shape=(None,),name="decoder_input")

decoder_emb = Embedding(ass_vocab, emb_dim, mask_zero=True, name="decoder_embedding")(decoder_inputs)
d_lstm_out,d_h,d_c = LSTM(128, return_state=True, return_sequences=True, name="decoder_lstm_1")(decoder_emb, initial_state=encoder_states)
#d_lstm_out1 = LSTM(latent_dim, return_state=False, return_sequences=True,name="decoder_lstm_2")(d_lstm_out, initial_state=context_second)
decoder_dense = keras.layers.Dense(ass_vocab, activation="softmax")

decoder_outputs = decoder_dense(d_lstm_out)

model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
loss = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9)

model.compile(optimizer=optimizer, loss=loss, metrics=["accuracy"])
model.summary()

roman_padded[45]

roman_words[45]

ass_padded[45]

y_train_padded[53587]

batch_size = 64
epochs = 100


model.fit(
    [roman_padded, ass_padded],
    y_train_padded,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.2,
)
model.save("/content/drive/MyDrive/internship/mt_bilstm_1l.h5")

#Load saved model
saved_model = tf.keras.models.load_model('/content/drive/MyDrive/internship/mt_bilstm_1l.h5')
saved_model.summary()

inputs = saved_model.get_layer('encoder_input').output                    #Encoder input
bi_lstm_out,f_h,f_c,b_h,b_c = saved_model.get_layer('BiLSTM').output   #Encoder LSTM output

concate_h = saved_model.get_layer('concat_h').output
concate_c = saved_model.get_layer('concat_c').output

bi_lstm_out

targets = saved_model.get_layer('decoder_input').output           #Decoder input
embedding_layer = saved_model.get_layer('decoder_embedding')      #Decoder Embedding layer
decoder_lstm1 = saved_model.get_layer('decoder_lstm_1')           #Decoder LSTM layer
dense1 = saved_model.get_layer('dense_1')

#Encoder

encoder = keras.Model(inputs, [concate_h, concate_c])
encoder.summary()

#Decoder

decoder_input_h1 = Input(shape=(128,))
decoder_input_c1 = Input(shape=(128,))

x = embedding_layer(targets)

x

x,d_out_h,d_out_c = decoder_lstm1(x, initial_state=[decoder_input_h1, decoder_input_c1])

#x = decoder_lstm2(x, initial_state=[decoder_input_h1, decoder_input_c1])

x = dense1(x)
x.shape

decoder = keras.Model([targets]+[decoder_input_h1, decoder_input_c1] , [x] + [d_out_h, d_out_c])

decoder.summary()

reverse_input_char_index = dict((i, char) for char, i in roman_tokens.items())
reverse_target_char_index = dict((i, char) for char, i in ass_tokens.items())

reverse_input_char_index

reverse_target_char_index

v = np.reshape(roman_padded[4], (1, -1))

encoder.predict(v,verbose=0)

def predict_list_of_words(list_source_words_in_padded_int_seq):
  list_pred_words = []
  start_token = np.zeros((1, 1))
  start_token[0] = ass_tokenizer.word_index['\t']
  print(start_token)

  for x in tqdm(range(len(list_source_words_in_padded_int_seq))):
    source_seq = start_token
    pred_word = ''
    v = np.reshape(list_source_words_in_padded_int_seq[x], (1, -1))
    next_h, next_c = encoder.predict(v,verbose=0)
    for i in range(max_decoder_seq_length_ass):
      output, next_h, next_c = decoder.predict([source_seq] + [next_h, next_c],verbose=0)
      next_token = np.argmax(output[0, 0, :])
      next_char = reverse_target_char_index[next_token]
      if next_char == '\n':
        break
      else:
        pred_word += next_char
        source_seq = np.zeros((1, 1))
        source_seq[0] = next_token
    list_pred_words.append(pred_word)
  return list_pred_words

predicted = predict_list_of_words(roman_padded[:20])

df_analyse = pd.DataFrame()
df_analyse["source"] = roman_words[:20]
df_analyse["predicted"] = predicted
df_analyse["ground_truth"] = df_train["native word"]

df_analyse

test_roman_words = df_test['english word']
test_target_words = df_test['native word']
test_roman_words[2],test_target_words[2]

test_roman_words_in_ids = roman_tokenizer.texts_to_sequences(test_roman_words)
test_target_words_in_ids = ass_tokenizer.texts_to_sequences(test_target_words)

print(test_roman_words_in_ids[4])
print(test_target_words_in_ids[4])

#2. padding

test_roman_padded = post_padding(test_roman_words_in_ids, max_encoder_seq_length_roman)
test_target_padded = post_padding(test_target_words_in_ids, max_decoder_seq_length_ass)

print(test_roman_padded[2])
print(test_target_padded[2])

len(test_roman_padded[2]), len(test_target_padded[2])

# prediction on test set

predicted_test = predict_list_of_words(test_roman_padded[:1500])

# analyse

df_analyse_test = pd.DataFrame()
df_analyse_test["source"] = test_roman_words[:1500]
df_analyse_test["predicted"] = predicted_test
df_analyse_test["ground_truth_label"] = df_test["native word"][:1500]

df_analyse_test.head(10)

# Calculate accuracy

accuracy = (df_analyse_test['predicted'] == df_analyse_test['ground_truth_label']).mean()

print(f'Test Accuracy = {accuracy*100:.2f}%')

def split_characters(text_string):
  return [character for character in text_string]

split_characters('চ' 'তু' 'ৰ্থ')

df_analyse_test["predicted_tokenized"] = df_analyse_test["predicted"].apply(split_characters)
df_analyse_test["label_tokenized"] = df_analyse_test["ground_truth_label"].apply(split_characters)

df_analyse_test.head()

def create_reference(tokenized):
  return [tokenized]

df_analyse_test["reference"] = df_analyse_test["label_tokenized"].apply(create_reference)
df_analyse_test.head()

# BLEU Score

# Convert DataFrame columns to lists
predictions = df_analyse_test['predicted_tokenized'].tolist()
true_labels = df_analyse_test['reference'].tolist()

true_labels

predictions

bleu = load_metric("bleu")
bleu.compute(predictions = predictions, references = true_labels)

def transliterate(source_word_in_padded_int_seq):
  #list_pred_words = []
  start_token = np.zeros((1, 1))
  start_token[0] = ass_tokenizer.word_index['\t']
  #print(start_token)

  #for x in tqdm(range(len(list_source_words_in_padded_int_seq))):
  source_seq = start_token
  pred_word = ''
  #v = np.reshape(source_word_in_padded_int_seq, (1, -1))
  next_h, next_c = encoder.predict(source_word_in_padded_int_seq,verbose=0)
  for i in range(max_decoder_seq_length_ass):
    output, next_h, next_c = decoder.predict([source_seq] + [next_h, next_c],verbose=0)
    next_token = np.argmax(output[0, 0, :])
    next_char = reverse_target_char_index[next_token]
    if next_char == '\n':
      break
    else:
      pred_word += next_char
      source_seq = np.zeros((1, 1))
      source_seq[0] = next_token
  #list_pred_words.append(pred_word)
  return pred_word

input_word = input("\nEnter a romanized word: ")
input_word_in_ids = roman_tokenizer.texts_to_sequences([input_word])

input_word_in_ids_padded = post_padding(input_word_in_ids, max_encoder_seq_length_roman)

print(f"\nTransliterated word : {transliterate(input_word_in_ids_padded)}")

